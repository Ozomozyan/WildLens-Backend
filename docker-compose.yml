services:
  ai:
    build:
      context: .               # ← use repo root so we can COPY both ai/ and api/
      dockerfile: ai/Dockerfile
    env_file: .env
    ports:
      - "8001:8001"
    volumes:
      - ./ai/:/app/ai          # mount your ai code under /app/ai
      - ./api/:/app/api        # mount your FastAPI code under /app/api
    networks:
      - wildlens
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  backend:
    build: .
    shm_size: "1g" 
    env_file: .env
    depends_on:
      ai:
        condition: service_healthy
    ports:
      - "8000:8000"
    environment:
      - AI_SERVICE_URL=http://ai:8001/predict
      - GUNICORN_CMD_ARGS=\
          --access-logfile /app/logs/gunicorn.log \
          --error-logfile  /app/logs/gunicorn.log \
          --capture-output \
          --log-level info          # or “debug” if you need more
    volumes:
      - ./:/app
      - ./logs:/app/logs  
      - static_data:/app/static
    networks:
      - wildlens
    restart: unless-stopped

networks:
  wildlens:

volumes:
  static_data:
